{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f794dbe8-d103-4925-8613-6a8109efd5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 19.4790\n",
      "\n",
      "Estimation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>mu0</th>\n",
       "      <th>std_mu0</th>\n",
       "      <th>mu1</th>\n",
       "      <th>std_mu1</th>\n",
       "      <th>sig0</th>\n",
       "      <th>std_sig0</th>\n",
       "      <th>sig1</th>\n",
       "      <th>std_sig1</th>\n",
       "      <th>ATE</th>\n",
       "      <th>ATE_std_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SciPy</td>\n",
       "      <td>-0.9895</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>1.0056</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>1.9972</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>19.2381</td>\n",
       "      <td>0.9654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAX</td>\n",
       "      <td>-0.9895</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1.0056</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>1.9973</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>19.2426</td>\n",
       "      <td>0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PyTorch</td>\n",
       "      <td>-0.9893</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1.0056</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>1.9966</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>19.1927</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Method     mu0  std_mu0    mu1  std_mu1   sig0  std_sig0   sig1  std_sig1  \\\n",
       "0    SciPy -0.9895   0.0143 0.9941   0.0288 1.0056    0.0103 1.9972    0.0199   \n",
       "1      JAX -0.9895   0.0143 0.9941   0.0281 1.0056    0.0101 1.9973    0.0199   \n",
       "2  PyTorch -0.9893   0.0143 0.9929   0.0281 1.0056    0.0101 1.9966    0.0199   \n",
       "\n",
       "      ATE  ATE_std_error  \n",
       "0 19.2381         0.9654  \n",
       "1 19.2426         0.9667  \n",
       "2 19.1927         0.9632  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from scipy.optimize import minimize\n",
    "from jax import grad, hessian\n",
    "from jax.scipy.stats import norm as jnorm\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== Data Generation =====================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Data generation\n",
    "N = 10000\n",
    "t_np = np.random.binomial(1, 0.5, N)\n",
    "e0_np = np.random.normal(0, 1, N)\n",
    "e1_np = np.random.normal(0, 1, N)\n",
    "a1 = a0 = -1\n",
    "tau = 2\n",
    "mu0_np = a0\n",
    "mu1_np = a1 + tau * t_np\n",
    "sig0 = 1\n",
    "sig1 = 2\n",
    "lny0_np = mu0_np + sig0 * e0_np\n",
    "lny1_np = mu1_np + sig1 * e1_np\n",
    "y0_np = np.exp(lny0_np)\n",
    "y1_np = np.exp(lny1_np)\n",
    "y_np = t_np * y1_np + (1 - t_np) * y0_np\n",
    "lny_np = np.round(np.log(y_np), 4)\n",
    "ate_true = np.exp(a1 + tau + 0.5 * sig1**2) - np.exp(a0 + 0.5 * sig0**2)\n",
    "print(f\"True ATE: {ate_true:.4f}\")\n",
    "\n",
    "# ===================== SciPy Estimation =====================\n",
    "\n",
    "def negloglik_scipy(theta):\n",
    "    mu0, mu1, log_sig0, log_sig1 = theta\n",
    "    sig0 = np.exp(log_sig0)\n",
    "    sig1 = np.exp(log_sig1)\n",
    "    lny0_ll = norm.logpdf(lny_np, loc=mu0, scale=sig0)\n",
    "    lny1_ll = norm.logpdf(lny_np, loc=mu1, scale=sig1)\n",
    "    loss = -np.sum(t_np * lny1_ll + (1 - t_np) * lny0_ll)\n",
    "    return loss\n",
    "\n",
    "# Initial parameter guesses\n",
    "theta0 = np.array([-1.0, -2.0, np.log(1.0), np.log(2.0)])\n",
    "\n",
    "# Optimization using BFGS without providing gradients (numerical differentiation)\n",
    "res_scipy = minimize(negloglik_scipy, theta0, method='BFGS', tol=1e-6)\n",
    "\n",
    "# Extract estimated parameters\n",
    "mu0_est_scipy, mu1_est_scipy = res_scipy.x[0], res_scipy.x[1]\n",
    "log_sig0_est_scipy, log_sig1_est_scipy = res_scipy.x[2], res_scipy.x[3]\n",
    "sig0_est_scipy = np.exp(log_sig0_est_scipy)\n",
    "sig1_est_scipy = np.exp(log_sig1_est_scipy)\n",
    "\n",
    "# Compute standard errors using the inverse Hessian\n",
    "V_scipy = res_scipy.hess_inv\n",
    "std_errors_scipy = np.sqrt(np.diag(V_scipy))\n",
    "var_log_sig0_scipy = V_scipy[2, 2]\n",
    "var_log_sig1_scipy = V_scipy[3, 3]\n",
    "std_sig0_est_scipy = sig0_est_scipy * np.sqrt(var_log_sig0_scipy)\n",
    "std_sig1_est_scipy = sig1_est_scipy * np.sqrt(var_log_sig1_scipy)\n",
    "\n",
    "# Compute ATE and its variance\n",
    "Ey0_scipy = np.exp(mu0_est_scipy + 0.5 * sig0_est_scipy**2)\n",
    "Ey1_scipy = np.exp(mu1_est_scipy + 0.5 * sig1_est_scipy**2)\n",
    "ate_est_scipy = Ey1_scipy - Ey0_scipy\n",
    "\n",
    "delta_g_scipy = np.array([\n",
    "    -Ey0_scipy,  # derivative w.r.t mu0\n",
    "    Ey1_scipy,   # derivative w.r.t mu1\n",
    "    -Ey0_scipy * sig0_est_scipy**2,  # derivative w.r.t log_sig0\n",
    "    Ey1_scipy * sig1_est_scipy**2    # derivative w.r.t log_sig1\n",
    "])\n",
    "\n",
    "V_ATE_scipy = delta_g_scipy.T @ V_scipy @ delta_g_scipy\n",
    "ate_std_error_scipy = np.sqrt(V_ATE_scipy)\n",
    "\n",
    "# ===================== JAX Estimation =====================\n",
    "\n",
    "# Convert data to JAX arrays\n",
    "t_jax = jnp.array(t_np)\n",
    "lny_jax = jnp.array(lny_np)\n",
    "\n",
    "def negloglik_jax(theta):\n",
    "    mu0, mu1, log_sig0, log_sig1 = theta\n",
    "    sig0 = jnp.exp(log_sig0)\n",
    "    sig1 = jnp.exp(log_sig1)\n",
    "    lny0_ll = jnorm.logpdf(lny_jax, loc=mu0, scale=sig0)\n",
    "    lny1_ll = jnorm.logpdf(lny_jax, loc=mu1, scale=sig1)\n",
    "    loss = -jnp.sum(t_jax * lny1_ll + (1 - t_jax) * lny0_ll)\n",
    "    return loss\n",
    "\n",
    "# Compute gradient and Hessian using JAX\n",
    "negloglik_grad_jax = grad(negloglik_jax)\n",
    "negloglik_hessian_jax = hessian(negloglik_jax)\n",
    "\n",
    "# Optimization using BFGS with gradient\n",
    "theta0_jax = np.array([-1.0, -2.0, np.log(1.0), np.log(2.0)])\n",
    "res_jax = minimize(negloglik_jax, theta0_jax, method='BFGS', jac=negloglik_grad_jax)\n",
    "\n",
    "# Extract estimated parameters\n",
    "mu0_est_jax, mu1_est_jax = res_jax.x[0], res_jax.x[1]\n",
    "log_sig0_est_jax, log_sig1_est_jax = res_jax.x[2], res_jax.x[3]\n",
    "sig0_est_jax = np.exp(log_sig0_est_jax)\n",
    "sig1_est_jax = np.exp(log_sig1_est_jax)\n",
    "\n",
    "# Compute standard errors using the Hessian\n",
    "H_jax = negloglik_hessian_jax(res_jax.x)\n",
    "V_jax = np.linalg.inv(H_jax)\n",
    "std_errors_jax = np.sqrt(np.diag(V_jax))\n",
    "var_log_sig0_jax = V_jax[2, 2]\n",
    "var_log_sig1_jax = V_jax[3, 3]\n",
    "std_sig0_est_jax = sig0_est_jax * np.sqrt(var_log_sig0_jax)\n",
    "std_sig1_est_jax = sig1_est_jax * np.sqrt(var_log_sig1_jax)\n",
    "\n",
    "# Compute ATE and its variance\n",
    "Ey0_jax = np.exp(mu0_est_jax + 0.5 * sig0_est_jax**2)\n",
    "Ey1_jax = np.exp(mu1_est_jax + 0.5 * sig1_est_jax**2)\n",
    "ate_est_jax = Ey1_jax - Ey0_jax\n",
    "\n",
    "delta_g_jax = np.array([\n",
    "    -Ey0_jax,  # derivative w.r.t mu0\n",
    "    Ey1_jax,   # derivative w.r.t mu1\n",
    "    -Ey0_jax * sig0_est_jax**2,  # derivative w.r.t log_sig0\n",
    "    Ey1_jax * sig1_est_jax**2    # derivative w.r.t log_sig1\n",
    "])\n",
    "\n",
    "V_ATE_jax = delta_g_jax.T @ V_jax @ delta_g_jax\n",
    "ate_std_error_jax = np.sqrt(V_ATE_jax)\n",
    "\n",
    "# ===================== PyTorch Estimation =====================\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "t_torch = torch.tensor(t_np, dtype=torch.float32)\n",
    "lny_torch = torch.tensor(lny_np, dtype=torch.float32)\n",
    "\n",
    "def negloglik_torch(theta):\n",
    "    mu0, mu1, log_sig0, log_sig1 = theta\n",
    "    sig0 = torch.exp(log_sig0)\n",
    "    sig1 = torch.exp(log_sig1)\n",
    "    lny0_dist = torch.distributions.Normal(mu0, sig0)\n",
    "    lny1_dist = torch.distributions.Normal(mu1, sig1)\n",
    "    lny0_ll = lny0_dist.log_prob(lny_torch)\n",
    "    lny1_ll = lny1_dist.log_prob(lny_torch)\n",
    "    loss = -torch.sum(t_torch * lny1_ll + (1 - t_torch) * lny0_ll)\n",
    "    return loss\n",
    "\n",
    "# Initial parameter guesses\n",
    "theta0_torch = torch.tensor([-1.0, -2.0, np.log(1.0), np.log(2.0)], requires_grad=True)\n",
    "\n",
    "# Optimization using LBFGS\n",
    "optimizer = optim.LBFGS([theta0_torch], max_iter=100, line_search_fn='strong_wolfe')\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = negloglik_torch(theta0_torch)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)\n",
    "\n",
    "# Extract estimated parameters\n",
    "mu0_est_torch = theta0_torch[0].item()\n",
    "mu1_est_torch = theta0_torch[1].item()\n",
    "log_sig0_est_torch = theta0_torch[2].item()\n",
    "log_sig1_est_torch = theta0_torch[3].item()\n",
    "sig0_est_torch = np.exp(log_sig0_est_torch)\n",
    "sig1_est_torch = np.exp(log_sig1_est_torch)\n",
    "\n",
    "# Compute standard errors using Hessian\n",
    "def negloglik_torch_vector(theta_vector):\n",
    "    return negloglik_torch(theta_vector)\n",
    "\n",
    "theta_opt = theta0_torch.detach().clone().requires_grad_(True)\n",
    "hessian_torch = torch.autograd.functional.hessian(negloglik_torch_vector, theta_opt)\n",
    "H_torch = hessian_torch.detach().numpy()\n",
    "V_torch = np.linalg.inv(H_torch)\n",
    "std_errors_torch = np.sqrt(np.diag(V_torch))\n",
    "var_log_sig0_torch = V_torch[2, 2]\n",
    "var_log_sig1_torch = V_torch[3, 3]\n",
    "std_sig0_est_torch = sig0_est_torch * np.sqrt(var_log_sig0_torch)\n",
    "std_sig1_est_torch = sig1_est_torch * np.sqrt(var_log_sig1_torch)\n",
    "\n",
    "# Compute ATE and its variance\n",
    "Ey0_torch = np.exp(mu0_est_torch + 0.5 * sig0_est_torch**2)\n",
    "Ey1_torch = np.exp(mu1_est_torch + 0.5 * sig1_est_torch**2)\n",
    "ate_est_torch = Ey1_torch - Ey0_torch\n",
    "\n",
    "delta_g_torch = np.array([\n",
    "    -Ey0_torch,  # derivative w.r.t mu0\n",
    "    Ey1_torch,   # derivative w.r.t mu1\n",
    "    -Ey0_torch * sig0_est_torch**2,  # derivative w.r.t log_sig0\n",
    "    Ey1_torch * sig1_est_torch**2    # derivative w.r.t log_sig1\n",
    "])\n",
    "\n",
    "V_ATE_torch = delta_g_torch.T @ V_torch @ delta_g_torch\n",
    "ate_std_error_torch = np.sqrt(V_ATE_torch)\n",
    "\n",
    "# ===================== Compile Results =====================\n",
    "\n",
    "results = {\n",
    "    'Method': ['SciPy', 'JAX', 'PyTorch'],\n",
    "    'mu0': [mu0_est_scipy, mu0_est_jax, mu0_est_torch],\n",
    "    'std_mu0': [std_errors_scipy[0], std_errors_jax[0], std_errors_torch[0]],\n",
    "    'mu1': [mu1_est_scipy, mu1_est_jax, mu1_est_torch],\n",
    "    'std_mu1': [std_errors_scipy[1], std_errors_jax[1], std_errors_torch[1]],\n",
    "    'sig0': [sig0_est_scipy, sig0_est_jax, sig0_est_torch],\n",
    "    'std_sig0': [std_sig0_est_scipy, std_sig0_est_jax, std_sig0_est_torch],\n",
    "    'sig1': [sig1_est_scipy, sig1_est_jax, sig1_est_torch],\n",
    "    'std_sig1': [std_sig1_est_scipy, std_sig1_est_jax, std_sig1_est_torch],\n",
    "    'ATE': [ate_est_scipy, ate_est_jax, ate_est_torch],\n",
    "    'ATE_std_error': [ate_std_error_scipy, ate_std_error_jax, ate_std_error_torch]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Format the numbers for better readability\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nEstimation Results:\")\n",
    "display(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
