{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cea4f-fd96-4bce-a97d-1f1bf8fe424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running Simulation for Option 1: Trigonometric and Exponential Terms\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_replications = 30   # Number of simulation replications\n",
    "n_samples = 1000       # Number of observations per replication\n",
    "n_features = 500       # Number of covariates (high-dimensional X)\n",
    "theta_true = 0.5       # True Average Treatment Effect (ATE)\n",
    "\n",
    "# Function to generate covariance matrix Σ with entries Σ_{kj} = 0.7^{|j-k|}\n",
    "def generate_covariance_matrix(n_features, rho=0.7):\n",
    "    indices = np.arange(n_features)\n",
    "    cov_matrix = rho ** np.abs(np.subtract.outer(indices, indices))\n",
    "    return cov_matrix\n",
    "\n",
    "# Define DGP Options\n",
    "dgps = [\n",
    "    {\n",
    "        'name': 'Option 1: Trigonometric and Exponential Terms',\n",
    "        'm_0': lambda x: np.sin(x[:, 0]) + np.log(np.abs(x[:, 2]) + 1) / (1 + np.exp(-x[:, 1])),\n",
    "        'g_0': lambda x: np.cos(x[:, 0] * x[:, 1]) + np.exp(-np.abs(x[:, 3])) + x[:, 4]**2 / (1 + np.abs(x[:, 5]))\n",
    "    },\n",
    "    {\n",
    "        'name': 'Option 2: Sparse Interactions with Polynomial Growth',\n",
    "        'm_0': lambda x: x[:, 0]**2 + 0.5 * x[:, 1]**3 - np.sqrt(np.abs(x[:, 2])) + 0.25 * x[:, 3] * x[:, 4],\n",
    "        'g_0': lambda x: 0.5 * x[:, 0] * x[:, 1]**2 - np.log(np.abs(x[:, 2]) + 1) + x[:, 3]**3 - 0.25 * x[:, 4]**2 * x[:, 5]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Option 3: Noisy Signal with Latent Periodicity',\n",
    "        'm_0': lambda x: np.sin(2 * np.pi * x[:, 0]) + np.cos(2 * np.pi * x[:, 1]) + 0.1 * x[:, 2]**2 + np.random.normal(0, 0.1, x.shape[0]),\n",
    "        'g_0': lambda x: np.cos(x[:, 0] + x[:, 1]) + np.sin(x[:, 2] - x[:, 3]) + np.exp(-np.abs(x[:, 4])) + np.random.normal(0, 0.1, x.shape[0])\n",
    "    },\n",
    "    {\n",
    "        'name': 'Option 4: Sparse but High Dimensional',\n",
    "        'm_0': lambda x: np.sum([x[:, j] / (1 + np.abs(x[:, j + 5])) for j in range(5)], axis=0) + np.log(1 + np.abs(x[:, 10])),\n",
    "        'g_0': lambda x: np.sum([np.sin(x[:, j]) / (1 + np.exp(x[:, j + 3])) for j in range(3)], axis=0) + 1 / np.sqrt(np.abs(x[:, 15]) + 1)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Option 5: Non-Linear Feature Combinations',\n",
    "        'm_0': lambda x: (x[:, 0] * x[:, 1]) / (1 + np.abs(x[:, 2])) + np.exp(-np.abs(x[:, 3])) + np.log(1 + np.abs(x[:, 4])) / (1 + np.abs(x[:, 5])),\n",
    "        'g_0': lambda x: np.sin(x[:, 0] * x[:, 1]) + np.cos(x[:, 2] - x[:, 3]) + x[:, 4]**2 / (1 + np.exp(x[:, 5]))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define Estimators\n",
    "def ols_estimator(y, d, X):\n",
    "    X_design = np.column_stack((d, X))\n",
    "    model = LinearRegression().fit(X_design, y)\n",
    "    theta_hat = model.coef_[0]\n",
    "    return theta_hat\n",
    "\n",
    "def naive_ml_estimator(y, d, X):\n",
    "    # LightGBM without regularization\n",
    "    lgb_params = {'num_leaves': 31, 'learning_rate': 0.1, 'verbose': -1}\n",
    "    lgb_y = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_d = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_y.fit(X, y)\n",
    "    lgb_d.fit(X, d)\n",
    "    M_y_X = lgb_y.predict(X)\n",
    "    M_d_X = lgb_d.predict(X)\n",
    "    y_tilde = y - M_y_X\n",
    "    d_tilde = d - M_d_X\n",
    "    # Regress y_tilde on d_tilde\n",
    "    model = LinearRegression().fit(d_tilde.reshape(-1,1), y_tilde)\n",
    "    theta_hat = model.coef_[0]\n",
    "    return theta_hat\n",
    "\n",
    "def regularized_ml_estimator(y, d, X):\n",
    "    # LightGBM with regularization\n",
    "    lgb_params = {'num_leaves': 10, 'learning_rate': 0.1, 'verbose': -1,\n",
    "                  'min_data_in_leaf': 20, 'max_depth': 5}\n",
    "    lgb_y = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_d = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_y.fit(X, y)\n",
    "    lgb_d.fit(X, d)\n",
    "    M_y_X = lgb_y.predict(X)\n",
    "    M_d_X = lgb_d.predict(X)\n",
    "    y_tilde = y - M_y_X\n",
    "    d_tilde = d - M_d_X\n",
    "    # Regress y_tilde on d_tilde\n",
    "    model = LinearRegression().fit(d_tilde.reshape(-1,1), y_tilde)\n",
    "    theta_hat = model.coef_[0]\n",
    "    return theta_hat\n",
    "\n",
    "def double_ml_estimator(y, d, X, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    theta_list = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        d_train, d_test = d[train_index], d[test_index]\n",
    "        # Estimate M_y(X) and M_d(X) on training data\n",
    "        lgb_params = {'num_leaves': 10, 'learning_rate': 0.1, 'verbose': -1,\n",
    "                      'min_data_in_leaf': 20, 'max_depth': 5}\n",
    "        lgb_y = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_d = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_y.fit(X_train, y_train)\n",
    "        lgb_d.fit(X_train, d_train)\n",
    "        # Predict on test data\n",
    "        M_y_X = lgb_y.predict(X_test)\n",
    "        M_d_X = lgb_d.predict(X_test)\n",
    "        y_tilde = y_test - M_y_X\n",
    "        d_tilde = d_test - M_d_X\n",
    "        # Regress y_tilde on d_tilde in test data\n",
    "        model = LinearRegression().fit(d_tilde.reshape(-1,1), y_tilde)\n",
    "        theta_hat_fold = model.coef_[0]\n",
    "        theta_list.append(theta_hat_fold)\n",
    "    # Average theta estimates over folds\n",
    "    theta_hat = np.mean(theta_list)\n",
    "    return theta_hat\n",
    "\n",
    "# Function to run simulation for a single DGP\n",
    "def run_simulation(dgp, dgp_index):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Simulation for {dgp['name']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Initialize storage for estimator results\n",
    "    results = {\n",
    "        'OLS': np.empty(n_replications),\n",
    "        'Naive_ML': np.empty(n_replications),\n",
    "        'Reg_ML': np.empty(n_replications),\n",
    "        'Double_ML': np.empty(n_replications)\n",
    "    }\n",
    "    \n",
    "    # Simulation Loop\n",
    "    for i in range(n_replications):\n",
    "        # Generate data\n",
    "        cov_matrix = generate_covariance_matrix(n_features)\n",
    "        X = np.random.multivariate_normal(mean=np.zeros(n_features), cov=cov_matrix, size=n_samples)\n",
    "        v_i = np.random.normal(0, 1, n_samples)\n",
    "        zeta_i = np.random.normal(0, 1, n_samples)\n",
    "        d_i = dgp['m_0'](X) + v_i\n",
    "        y_i = theta_true * d_i + dgp['g_0'](X) + zeta_i\n",
    "        \n",
    "        # Apply Estimators\n",
    "        results['OLS'][i] = ols_estimator(y_i, d_i, X)\n",
    "        results['Naive_ML'][i] = naive_ml_estimator(y_i, d_i, X)\n",
    "        results['Reg_ML'][i] = regularized_ml_estimator(y_i, d_i, X)\n",
    "        results['Double_ML'][i] = double_ml_estimator(y_i, d_i, X)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Replication {i+1}/{n_replications} completed.\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plotting the Sampling Distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.kdeplot(results_df['OLS'], label='OLS Estimator', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(results_df['Naive_ML'], label='Naive ML Estimator', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(results_df['Reg_ML'], label='Regularized ML Estimator', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(results_df['Double_ML'], label='Double ML Estimator', fill=True, alpha=0.5)\n",
    "    plt.axvline(theta_true, color='black', linestyle='--', label='True ATE')\n",
    "    plt.title(f'Sampling Distributions of ATE Estimators - {dgp[\"name\"]}')\n",
    "    plt.xlabel('Estimated ATE')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.xlim(theta_true - 1, theta_true + 1)\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary Statistics\n",
    "    summary_stats = results_df.describe().loc[['mean', 'std']].transpose()\n",
    "    summary_stats['Bias'] = summary_stats['mean'] - theta_true\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary_stats)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Run simulations for all DGPs\n",
    "for idx, dgp in enumerate(dgps):\n",
    "    run_simulation(dgp, idx+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02b6ab-6da2-4e0e-828c-59b7500722de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
